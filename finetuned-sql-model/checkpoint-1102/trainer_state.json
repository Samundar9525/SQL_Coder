{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 1102,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.018148820326678767,
      "grad_norm": 37.131080627441406,
      "learning_rate": 0.00019836660617059894,
      "loss": 8.5993,
      "step": 10
    },
    {
      "epoch": 0.036297640653357534,
      "grad_norm": 2.3884494304656982,
      "learning_rate": 0.00019655172413793104,
      "loss": 1.4999,
      "step": 20
    },
    {
      "epoch": 0.0544464609800363,
      "grad_norm": 1.827115535736084,
      "learning_rate": 0.00019473684210526317,
      "loss": 0.7344,
      "step": 30
    },
    {
      "epoch": 0.07259528130671507,
      "grad_norm": 1.1943764686584473,
      "learning_rate": 0.0001929219600725953,
      "loss": 0.5796,
      "step": 40
    },
    {
      "epoch": 0.09074410163339383,
      "grad_norm": 1.8590632677078247,
      "learning_rate": 0.00019110707803992742,
      "loss": 0.5048,
      "step": 50
    },
    {
      "epoch": 0.1088929219600726,
      "grad_norm": 3.722412586212158,
      "learning_rate": 0.00018929219600725953,
      "loss": 0.4617,
      "step": 60
    },
    {
      "epoch": 0.12704174228675136,
      "grad_norm": 1.2264373302459717,
      "learning_rate": 0.00018747731397459165,
      "loss": 0.387,
      "step": 70
    },
    {
      "epoch": 0.14519056261343014,
      "grad_norm": 2.0767974853515625,
      "learning_rate": 0.00018566243194192378,
      "loss": 0.3507,
      "step": 80
    },
    {
      "epoch": 0.16333938294010888,
      "grad_norm": 1.3545273542404175,
      "learning_rate": 0.0001838475499092559,
      "loss": 0.2643,
      "step": 90
    },
    {
      "epoch": 0.18148820326678766,
      "grad_norm": 1.614495873451233,
      "learning_rate": 0.00018203266787658804,
      "loss": 0.3261,
      "step": 100
    },
    {
      "epoch": 0.1996370235934664,
      "grad_norm": 0.9908018112182617,
      "learning_rate": 0.00018021778584392014,
      "loss": 0.2052,
      "step": 110
    },
    {
      "epoch": 0.2177858439201452,
      "grad_norm": 1.2927987575531006,
      "learning_rate": 0.0001784029038112523,
      "loss": 0.221,
      "step": 120
    },
    {
      "epoch": 0.23593466424682397,
      "grad_norm": 2.234844923019409,
      "learning_rate": 0.0001765880217785844,
      "loss": 0.2243,
      "step": 130
    },
    {
      "epoch": 0.2540834845735027,
      "grad_norm": 1.0992835760116577,
      "learning_rate": 0.0001747731397459165,
      "loss": 0.1687,
      "step": 140
    },
    {
      "epoch": 0.27223230490018147,
      "grad_norm": 3.3624510765075684,
      "learning_rate": 0.00017295825771324866,
      "loss": 0.2858,
      "step": 150
    },
    {
      "epoch": 0.29038112522686027,
      "grad_norm": 1.0199514627456665,
      "learning_rate": 0.00017114337568058076,
      "loss": 0.1968,
      "step": 160
    },
    {
      "epoch": 0.308529945553539,
      "grad_norm": 1.993578553199768,
      "learning_rate": 0.0001693284936479129,
      "loss": 0.1621,
      "step": 170
    },
    {
      "epoch": 0.32667876588021777,
      "grad_norm": 1.3287705183029175,
      "learning_rate": 0.00016751361161524502,
      "loss": 0.1925,
      "step": 180
    },
    {
      "epoch": 0.3448275862068966,
      "grad_norm": 1.6029386520385742,
      "learning_rate": 0.00016569872958257714,
      "loss": 0.1623,
      "step": 190
    },
    {
      "epoch": 0.3629764065335753,
      "grad_norm": 1.7578330039978027,
      "learning_rate": 0.00016388384754990927,
      "loss": 0.2697,
      "step": 200
    },
    {
      "epoch": 0.3811252268602541,
      "grad_norm": 3.6771697998046875,
      "learning_rate": 0.00016206896551724137,
      "loss": 0.2443,
      "step": 210
    },
    {
      "epoch": 0.3992740471869328,
      "grad_norm": 2.3265023231506348,
      "learning_rate": 0.0001602540834845735,
      "loss": 0.2548,
      "step": 220
    },
    {
      "epoch": 0.41742286751361163,
      "grad_norm": 1.4389221668243408,
      "learning_rate": 0.00015843920145190563,
      "loss": 0.208,
      "step": 230
    },
    {
      "epoch": 0.4355716878402904,
      "grad_norm": 1.3333741426467896,
      "learning_rate": 0.00015662431941923776,
      "loss": 0.225,
      "step": 240
    },
    {
      "epoch": 0.4537205081669691,
      "grad_norm": 1.3615630865097046,
      "learning_rate": 0.0001548094373865699,
      "loss": 0.209,
      "step": 250
    },
    {
      "epoch": 0.47186932849364793,
      "grad_norm": 2.934326171875,
      "learning_rate": 0.00015299455535390202,
      "loss": 0.2149,
      "step": 260
    },
    {
      "epoch": 0.4900181488203267,
      "grad_norm": 1.7879879474639893,
      "learning_rate": 0.00015117967332123412,
      "loss": 0.2315,
      "step": 270
    },
    {
      "epoch": 0.5081669691470054,
      "grad_norm": 1.3167095184326172,
      "learning_rate": 0.00014936479128856625,
      "loss": 0.1728,
      "step": 280
    },
    {
      "epoch": 0.5263157894736842,
      "grad_norm": 3.008399724960327,
      "learning_rate": 0.00014754990925589838,
      "loss": 0.2039,
      "step": 290
    },
    {
      "epoch": 0.5444646098003629,
      "grad_norm": 2.217989683151245,
      "learning_rate": 0.00014573502722323048,
      "loss": 0.1857,
      "step": 300
    },
    {
      "epoch": 0.5626134301270418,
      "grad_norm": 1.0672112703323364,
      "learning_rate": 0.00014392014519056263,
      "loss": 0.1211,
      "step": 310
    },
    {
      "epoch": 0.5807622504537205,
      "grad_norm": 1.3485075235366821,
      "learning_rate": 0.00014210526315789474,
      "loss": 0.2307,
      "step": 320
    },
    {
      "epoch": 0.5989110707803993,
      "grad_norm": 1.7011200189590454,
      "learning_rate": 0.00014029038112522686,
      "loss": 0.234,
      "step": 330
    },
    {
      "epoch": 0.617059891107078,
      "grad_norm": 2.127957820892334,
      "learning_rate": 0.000138475499092559,
      "loss": 0.1676,
      "step": 340
    },
    {
      "epoch": 0.6352087114337568,
      "grad_norm": 1.2497788667678833,
      "learning_rate": 0.0001366606170598911,
      "loss": 0.1813,
      "step": 350
    },
    {
      "epoch": 0.6533575317604355,
      "grad_norm": 1.5164506435394287,
      "learning_rate": 0.00013484573502722325,
      "loss": 0.2504,
      "step": 360
    },
    {
      "epoch": 0.6715063520871143,
      "grad_norm": 1.4737104177474976,
      "learning_rate": 0.00013303085299455535,
      "loss": 0.225,
      "step": 370
    },
    {
      "epoch": 0.6896551724137931,
      "grad_norm": 1.943002462387085,
      "learning_rate": 0.00013121597096188748,
      "loss": 0.168,
      "step": 380
    },
    {
      "epoch": 0.7078039927404719,
      "grad_norm": 1.468214511871338,
      "learning_rate": 0.0001294010889292196,
      "loss": 0.1839,
      "step": 390
    },
    {
      "epoch": 0.7259528130671506,
      "grad_norm": 3.4865076541900635,
      "learning_rate": 0.00012758620689655174,
      "loss": 0.2164,
      "step": 400
    },
    {
      "epoch": 0.7441016333938294,
      "grad_norm": 1.4004415273666382,
      "learning_rate": 0.00012577132486388387,
      "loss": 0.1587,
      "step": 410
    },
    {
      "epoch": 0.7622504537205081,
      "grad_norm": 1.348684310913086,
      "learning_rate": 0.00012395644283121597,
      "loss": 0.2335,
      "step": 420
    },
    {
      "epoch": 0.7803992740471869,
      "grad_norm": 2.7417263984680176,
      "learning_rate": 0.0001221415607985481,
      "loss": 0.2024,
      "step": 430
    },
    {
      "epoch": 0.7985480943738656,
      "grad_norm": 1.4120402336120605,
      "learning_rate": 0.00012032667876588021,
      "loss": 0.198,
      "step": 440
    },
    {
      "epoch": 0.8166969147005445,
      "grad_norm": 1.6770092248916626,
      "learning_rate": 0.00011851179673321235,
      "loss": 0.1393,
      "step": 450
    },
    {
      "epoch": 0.8348457350272233,
      "grad_norm": 1.3208582401275635,
      "learning_rate": 0.00011669691470054447,
      "loss": 0.1956,
      "step": 460
    },
    {
      "epoch": 0.852994555353902,
      "grad_norm": 3.050349235534668,
      "learning_rate": 0.0001148820326678766,
      "loss": 0.1801,
      "step": 470
    },
    {
      "epoch": 0.8711433756805808,
      "grad_norm": 1.0195660591125488,
      "learning_rate": 0.00011306715063520871,
      "loss": 0.169,
      "step": 480
    },
    {
      "epoch": 0.8892921960072595,
      "grad_norm": 2.7024741172790527,
      "learning_rate": 0.00011125226860254083,
      "loss": 0.2498,
      "step": 490
    },
    {
      "epoch": 0.9074410163339383,
      "grad_norm": 2.0534355640411377,
      "learning_rate": 0.00010943738656987297,
      "loss": 0.112,
      "step": 500
    },
    {
      "epoch": 0.925589836660617,
      "grad_norm": 2.0716962814331055,
      "learning_rate": 0.00010762250453720509,
      "loss": 0.2577,
      "step": 510
    },
    {
      "epoch": 0.9437386569872959,
      "grad_norm": 1.2786349058151245,
      "learning_rate": 0.00010580762250453721,
      "loss": 0.181,
      "step": 520
    },
    {
      "epoch": 0.9618874773139746,
      "grad_norm": 1.346339464187622,
      "learning_rate": 0.00010399274047186933,
      "loss": 0.1554,
      "step": 530
    },
    {
      "epoch": 0.9800362976406534,
      "grad_norm": 1.5895031690597534,
      "learning_rate": 0.00010217785843920144,
      "loss": 0.1649,
      "step": 540
    },
    {
      "epoch": 0.9981851179673321,
      "grad_norm": 1.8011417388916016,
      "learning_rate": 0.00010036297640653359,
      "loss": 0.164,
      "step": 550
    },
    {
      "epoch": 1.0163339382940109,
      "grad_norm": 1.7706173658370972,
      "learning_rate": 9.85480943738657e-05,
      "loss": 0.1546,
      "step": 560
    },
    {
      "epoch": 1.0344827586206897,
      "grad_norm": 1.1438149213790894,
      "learning_rate": 9.673321234119783e-05,
      "loss": 0.1435,
      "step": 570
    },
    {
      "epoch": 1.0526315789473684,
      "grad_norm": 2.1052684783935547,
      "learning_rate": 9.491833030852995e-05,
      "loss": 0.1015,
      "step": 580
    },
    {
      "epoch": 1.0707803992740472,
      "grad_norm": 2.7503252029418945,
      "learning_rate": 9.310344827586207e-05,
      "loss": 0.186,
      "step": 590
    },
    {
      "epoch": 1.0889292196007259,
      "grad_norm": 1.7981690168380737,
      "learning_rate": 9.128856624319419e-05,
      "loss": 0.1673,
      "step": 600
    },
    {
      "epoch": 1.1070780399274047,
      "grad_norm": 2.591207981109619,
      "learning_rate": 8.947368421052632e-05,
      "loss": 0.1407,
      "step": 610
    },
    {
      "epoch": 1.1252268602540836,
      "grad_norm": 1.8690146207809448,
      "learning_rate": 8.765880217785845e-05,
      "loss": 0.1892,
      "step": 620
    },
    {
      "epoch": 1.1433756805807622,
      "grad_norm": 1.7656481266021729,
      "learning_rate": 8.584392014519058e-05,
      "loss": 0.1066,
      "step": 630
    },
    {
      "epoch": 1.161524500907441,
      "grad_norm": 1.0090092420578003,
      "learning_rate": 8.402903811252269e-05,
      "loss": 0.1069,
      "step": 640
    },
    {
      "epoch": 1.1796733212341197,
      "grad_norm": 1.6743345260620117,
      "learning_rate": 8.22141560798548e-05,
      "loss": 0.1067,
      "step": 650
    },
    {
      "epoch": 1.1978221415607986,
      "grad_norm": 4.049194812774658,
      "learning_rate": 8.039927404718693e-05,
      "loss": 0.188,
      "step": 660
    },
    {
      "epoch": 1.2159709618874772,
      "grad_norm": 1.592792272567749,
      "learning_rate": 7.858439201451906e-05,
      "loss": 0.1069,
      "step": 670
    },
    {
      "epoch": 1.234119782214156,
      "grad_norm": 0.9023745059967041,
      "learning_rate": 7.676950998185118e-05,
      "loss": 0.1388,
      "step": 680
    },
    {
      "epoch": 1.252268602540835,
      "grad_norm": 1.594271183013916,
      "learning_rate": 7.495462794918331e-05,
      "loss": 0.1869,
      "step": 690
    },
    {
      "epoch": 1.2704174228675136,
      "grad_norm": 1.3737610578536987,
      "learning_rate": 7.313974591651544e-05,
      "loss": 0.121,
      "step": 700
    },
    {
      "epoch": 1.2885662431941924,
      "grad_norm": 1.2310209274291992,
      "learning_rate": 7.132486388384755e-05,
      "loss": 0.1732,
      "step": 710
    },
    {
      "epoch": 1.306715063520871,
      "grad_norm": 1.0356029272079468,
      "learning_rate": 6.950998185117967e-05,
      "loss": 0.1092,
      "step": 720
    },
    {
      "epoch": 1.32486388384755,
      "grad_norm": 1.6725353002548218,
      "learning_rate": 6.76950998185118e-05,
      "loss": 0.0992,
      "step": 730
    },
    {
      "epoch": 1.3430127041742286,
      "grad_norm": 2.736175060272217,
      "learning_rate": 6.588021778584392e-05,
      "loss": 0.1019,
      "step": 740
    },
    {
      "epoch": 1.3611615245009074,
      "grad_norm": 1.6742854118347168,
      "learning_rate": 6.406533575317605e-05,
      "loss": 0.1544,
      "step": 750
    },
    {
      "epoch": 1.3793103448275863,
      "grad_norm": 1.160244345664978,
      "learning_rate": 6.225045372050817e-05,
      "loss": 0.1586,
      "step": 760
    },
    {
      "epoch": 1.397459165154265,
      "grad_norm": 2.357365846633911,
      "learning_rate": 6.0435571687840296e-05,
      "loss": 0.149,
      "step": 770
    },
    {
      "epoch": 1.4156079854809438,
      "grad_norm": 2.3043603897094727,
      "learning_rate": 5.862068965517241e-05,
      "loss": 0.1444,
      "step": 780
    },
    {
      "epoch": 1.4337568058076224,
      "grad_norm": 2.1591522693634033,
      "learning_rate": 5.680580762250453e-05,
      "loss": 0.153,
      "step": 790
    },
    {
      "epoch": 1.4519056261343013,
      "grad_norm": 2.220320224761963,
      "learning_rate": 5.499092558983666e-05,
      "loss": 0.103,
      "step": 800
    },
    {
      "epoch": 1.47005444646098,
      "grad_norm": 2.8928797245025635,
      "learning_rate": 5.3176043557168784e-05,
      "loss": 0.1305,
      "step": 810
    },
    {
      "epoch": 1.4882032667876588,
      "grad_norm": 1.156646966934204,
      "learning_rate": 5.136116152450091e-05,
      "loss": 0.117,
      "step": 820
    },
    {
      "epoch": 1.5063520871143377,
      "grad_norm": 2.8315446376800537,
      "learning_rate": 4.954627949183303e-05,
      "loss": 0.1628,
      "step": 830
    },
    {
      "epoch": 1.5245009074410163,
      "grad_norm": 1.6396492719650269,
      "learning_rate": 4.7731397459165156e-05,
      "loss": 0.2169,
      "step": 840
    },
    {
      "epoch": 1.542649727767695,
      "grad_norm": 1.1013617515563965,
      "learning_rate": 4.591651542649728e-05,
      "loss": 0.1497,
      "step": 850
    },
    {
      "epoch": 1.560798548094374,
      "grad_norm": 1.0607659816741943,
      "learning_rate": 4.410163339382941e-05,
      "loss": 0.164,
      "step": 860
    },
    {
      "epoch": 1.5789473684210527,
      "grad_norm": 1.0326762199401855,
      "learning_rate": 4.228675136116152e-05,
      "loss": 0.2167,
      "step": 870
    },
    {
      "epoch": 1.5970961887477313,
      "grad_norm": 2.6816532611846924,
      "learning_rate": 4.047186932849365e-05,
      "loss": 0.1427,
      "step": 880
    },
    {
      "epoch": 1.6152450090744102,
      "grad_norm": 1.4388104677200317,
      "learning_rate": 3.865698729582577e-05,
      "loss": 0.0716,
      "step": 890
    },
    {
      "epoch": 1.633393829401089,
      "grad_norm": 3.085496664047241,
      "learning_rate": 3.6842105263157895e-05,
      "loss": 0.1302,
      "step": 900
    },
    {
      "epoch": 1.6515426497277677,
      "grad_norm": 2.238009214401245,
      "learning_rate": 3.502722323049002e-05,
      "loss": 0.1631,
      "step": 910
    },
    {
      "epoch": 1.6696914700544465,
      "grad_norm": 2.1503944396972656,
      "learning_rate": 3.3212341197822145e-05,
      "loss": 0.1491,
      "step": 920
    },
    {
      "epoch": 1.6878402903811254,
      "grad_norm": 1.2577502727508545,
      "learning_rate": 3.139745916515426e-05,
      "loss": 0.1401,
      "step": 930
    },
    {
      "epoch": 1.705989110707804,
      "grad_norm": 2.4028658866882324,
      "learning_rate": 2.958257713248639e-05,
      "loss": 0.1771,
      "step": 940
    },
    {
      "epoch": 1.7241379310344827,
      "grad_norm": 1.4836320877075195,
      "learning_rate": 2.7767695099818514e-05,
      "loss": 0.1286,
      "step": 950
    },
    {
      "epoch": 1.7422867513611615,
      "grad_norm": 1.160001277923584,
      "learning_rate": 2.595281306715064e-05,
      "loss": 0.0982,
      "step": 960
    },
    {
      "epoch": 1.7604355716878404,
      "grad_norm": 4.267240047454834,
      "learning_rate": 2.413793103448276e-05,
      "loss": 0.1667,
      "step": 970
    },
    {
      "epoch": 1.778584392014519,
      "grad_norm": 1.3912968635559082,
      "learning_rate": 2.2323049001814884e-05,
      "loss": 0.1111,
      "step": 980
    },
    {
      "epoch": 1.7967332123411979,
      "grad_norm": 3.5500283241271973,
      "learning_rate": 2.0508166969147005e-05,
      "loss": 0.1859,
      "step": 990
    },
    {
      "epoch": 1.8148820326678767,
      "grad_norm": 1.0598424673080444,
      "learning_rate": 1.869328493647913e-05,
      "loss": 0.0799,
      "step": 1000
    },
    {
      "epoch": 1.8330308529945554,
      "grad_norm": 1.5371859073638916,
      "learning_rate": 1.6878402903811253e-05,
      "loss": 0.1649,
      "step": 1010
    },
    {
      "epoch": 1.851179673321234,
      "grad_norm": 1.0167094469070435,
      "learning_rate": 1.5063520871143378e-05,
      "loss": 0.1136,
      "step": 1020
    },
    {
      "epoch": 1.8693284936479129,
      "grad_norm": 1.6359851360321045,
      "learning_rate": 1.32486388384755e-05,
      "loss": 0.1209,
      "step": 1030
    },
    {
      "epoch": 1.8874773139745917,
      "grad_norm": 1.4295198917388916,
      "learning_rate": 1.1433756805807623e-05,
      "loss": 0.17,
      "step": 1040
    },
    {
      "epoch": 1.9056261343012704,
      "grad_norm": 2.080174684524536,
      "learning_rate": 9.618874773139747e-06,
      "loss": 0.1256,
      "step": 1050
    },
    {
      "epoch": 1.9237749546279492,
      "grad_norm": 1.5230058431625366,
      "learning_rate": 7.80399274047187e-06,
      "loss": 0.1291,
      "step": 1060
    },
    {
      "epoch": 1.941923774954628,
      "grad_norm": 3.0076961517333984,
      "learning_rate": 5.9891107078039935e-06,
      "loss": 0.1111,
      "step": 1070
    },
    {
      "epoch": 1.9600725952813067,
      "grad_norm": 2.930990219116211,
      "learning_rate": 4.174228675136116e-06,
      "loss": 0.135,
      "step": 1080
    },
    {
      "epoch": 1.9782214156079854,
      "grad_norm": 0.8891823887825012,
      "learning_rate": 2.35934664246824e-06,
      "loss": 0.1509,
      "step": 1090
    },
    {
      "epoch": 1.9963702359346642,
      "grad_norm": 1.4555985927581787,
      "learning_rate": 5.44464609800363e-07,
      "loss": 0.1323,
      "step": 1100
    }
  ],
  "logging_steps": 10,
  "max_steps": 1102,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 876498385895424.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
